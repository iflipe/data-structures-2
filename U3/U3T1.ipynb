{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4201001f",
      "metadata": {
        "id": "4201001f"
      },
      "source": [
        "# Extraction and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l2Y8plhhfMk1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2Y8plhhfMk1",
        "outputId": "0591541a-562a-439c-ce0f-0a3fe8a39bd9"
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install EbookLib\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "2bfc2561",
      "metadata": {
        "id": "2bfc2561"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import spacy\n",
        "import ebooklib\n",
        "from ebooklib import epub\n",
        "from bs4 import BeautifulSoup\n",
        "from itertools import combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4Spj_6WiXUOY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Spj_6WiXUOY",
        "outputId": "c34d8f1e-19eb-4503-8234-7619865d1b8b"
      },
      "outputs": [],
      "source": [
        "# Read the epub file\n",
        "book = epub.read_epub('assets/GOT.epub')\n",
        "\n",
        "# Extract the text from the epub file\n",
        "docs = book.get_items_of_type(ebooklib.ITEM_DOCUMENT)\n",
        "chapters = []\n",
        "\n",
        "# Restrict the content to only the first book and keep the chapters separate\n",
        "for i, chapter in enumerate(docs):\n",
        "    chapters.append(chapter.get_body_content()) if i >= 7 else None\n",
        "    if i == 78:\n",
        "      break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "R1mEVfEgbyYk",
      "metadata": {
        "id": "R1mEVfEgbyYk"
      },
      "outputs": [],
      "source": [
        "# Get the chapter names and content and store them in separate lists\n",
        "chapter_titles = list(map(lambda x: BeautifulSoup(x, 'html.parser').get_text().splitlines()[1], chapters))\n",
        "chapter_contents = list(map(lambda x: BeautifulSoup(x, 'html.parser').get_text().split('\\n', 2)[2].replace('\\n', ' '), chapters))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d86a0479",
      "metadata": {
        "id": "d86a0479"
      },
      "source": [
        "# spaCy PoS-Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "51649227",
      "metadata": {
        "id": "51649227"
      },
      "outputs": [],
      "source": [
        "# Load the spaCy medium sized English model pipeline trained on written web text that includes vocabulary, syntax and entities.\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "dd9ad78d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_entities_pos(text):\n",
        "    \"\"\"\n",
        "    Extracts proper singular noun entities from the given text.\n",
        "    This function processes the input text using a natural language processing (NLP) pipeline,\n",
        "    separates the text into sentences, and then extracts tokens that are tagged as proper\n",
        "    singular nouns (NNP) from each sentence. The extracted entities are grouped by sentence\n",
        "    and returned as a list of lists.\n",
        "    Args:\n",
        "        text (str): The input text to be processed.\n",
        "    Returns:\n",
        "        List[List[str]]: A list of lists, where each inner list contains the proper singular\n",
        "        noun entities extracted from a sentence in the input text.\n",
        "    \"\"\"\n",
        "\n",
        "    # Process the text creating a Doc object that is a sequence of Token objects\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Separate the text in sentences\n",
        "    sentences = list(doc.sents)\n",
        "\n",
        "    entities = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "\n",
        "        sentence_entities = []\n",
        "\n",
        "        for token in sentence:\n",
        "            \n",
        "            # Check if the token is a proper singular noun and add it to the list of entities\n",
        "            if token.tag_ == 'NNP':\n",
        "\n",
        "                sentence_entities.append(token.text)\n",
        "\n",
        "        # Only add the list of entities if there is at least one entity in the sentence\n",
        "        if len(sentence_entities) > 0:\n",
        "\n",
        "            entities.append(sentence_entities)\n",
        "\n",
        "    return entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "zmzyI5B7E9Uf",
      "metadata": {
        "id": "zmzyI5B7E9Uf"
      },
      "outputs": [],
      "source": [
        "book_entities = []\n",
        "for chapter in chapter_contents:\n",
        "  book_entities.append(extract_entities_pos(chapter))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1e4d4f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "book_entities[0] # just for visualization purposes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f09fc6a",
      "metadata": {
        "id": "5f09fc6a"
      },
      "source": [
        "# spaCy Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8103c976",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_entities_ner(text):\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "    entities = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_entities = []\n",
        "        sent_doc = nlp(sentence.text)\n",
        "\n",
        "        for ent in sent_doc.ents:\n",
        "            if ent.label_ in ['PERSON', 'ORG', 'GPE']:\n",
        "                entity = ent.text.strip()\n",
        "\n",
        "                if entity.endswith((\"'s\",\"’s\")):\n",
        "                    entity = entity[:-2]\n",
        "                \n",
        "                if entity.startswith('“'):\n",
        "                    entity = entity[1:]\n",
        "\n",
        "                if entity != '':\n",
        "                    sentence_entities.append(entity)\n",
        "\n",
        "        sentence_entities = list(set(sentence_entities))\n",
        "\n",
        "        if len(sentence_entities) > 1:\n",
        "            entities.append(sentence_entities)\n",
        "\n",
        "    return entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "596b6054",
      "metadata": {},
      "outputs": [],
      "source": [
        "entities = []\n",
        "for chapter in chapter_contents:\n",
        "  entities.append(extract_entities_ner(chapter))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c16db77",
      "metadata": {},
      "outputs": [],
      "source": [
        "entities[0] # just for visualization purposes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef03c582",
      "metadata": {
        "id": "ef03c582"
      },
      "source": [
        "# Converting entities to network data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "54710214",
      "metadata": {
        "id": "54710214"
      },
      "outputs": [],
      "source": [
        "def get_network_data(entities, chapter_names):\n",
        "\n",
        "    final_sources = []\n",
        "    final_targets = []\n",
        "    final_chapters_names = []\n",
        "    for i, chapter in enumerate(entities):\n",
        "\n",
        "      for row in chapter:\n",
        "        pairs = combinations(row, 2)\n",
        "        for pair in pairs:\n",
        "\n",
        "          final_sources.append(pair[0])\n",
        "          final_targets.append(pair[1])\n",
        "          final_chapters_names.append(chapter_names[i])\n",
        "\n",
        "    df = pd.DataFrame({'source':final_sources, 'target':final_targets, 'chapter': final_chapters_names})\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "a2eeb612",
      "metadata": {
        "id": "a2eeb612"
      },
      "outputs": [],
      "source": [
        "got_network_df = get_network_data(entities, chapter_titles)\n",
        "\n",
        "got_network_df = got_network_df.groupby(got_network_df.columns.tolist(), observed=True).size().reset_index().rename(columns={0:'weight'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "JwzVh3aFDoF6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JwzVh3aFDoF6",
        "outputId": "60ce9223-621b-418d-a7dd-426349d9cf8c"
      },
      "outputs": [],
      "source": [
        "titles = got_network_df['chapter'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0cd4c54",
      "metadata": {
        "id": "d0cd4c54"
      },
      "source": [
        "# Converting network data into networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "7GbS_zMID8_w",
      "metadata": {
        "id": "7GbS_zMID8_w"
      },
      "outputs": [],
      "source": [
        "graphs = {}\n",
        "for title in got_network_df['chapter'].unique():\n",
        "    graphs[title] = nx.from_pandas_edgelist(got_network_df[got_network_df['chapter'] == title], edge_attr='weight')\n",
        "    nx.write_gexf(graphs[title], f'assets/got-{title}.gexf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "4blgcIenna6H",
      "metadata": {
        "id": "4blgcIenna6H"
      },
      "outputs": [],
      "source": [
        "multigraph = nx.from_pandas_edgelist(got_network_df, edge_attr=True, create_using=nx.MultiGraph())\n",
        "\n",
        "for source, target, values in multigraph.edges(data=True):\n",
        "    chapter = values['chapter']\n",
        "\n",
        "    if chapter not in multigraph.nodes[source].get('chapters',''):\n",
        "      text = \" \".join([multigraph.nodes[source].get('chapters',''), chapter])\n",
        "      multigraph.nodes[source]['chapters'] = text\n",
        "    if chapter not in multigraph.nodes[target].get('chapters',''):\n",
        "      text = \" \".join([multigraph.nodes[target].get('chapters',''), chapter])\n",
        "      multigraph.nodes[target]['chapters'] = text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "de6e7374",
      "metadata": {},
      "outputs": [],
      "source": [
        "global_data = {'Character': 'Global', 'Entities': nx.number_of_nodes(multigraph), 'Interactions' : nx.number_of_edges(multigraph),\n",
        "        'Graph Density': nx.density(multigraph), 'Connected Components': nx.number_connected_components(multigraph),\n",
        "        'Average Degree': sum([v for _, v in multigraph.degree])/nx.number_of_nodes(multigraph),\n",
        "        'Best Friends': \" and \".join(sorted(multigraph.edges(data=True), key = lambda x: x[2]['weight'], reverse=True)[0][:2])\n",
        "        }\n",
        "character_worlds = pd.DataFrame(global_data, index=[0])\n",
        "\n",
        "# Get info about each pov chapter\n",
        "for title in titles:\n",
        "    nodes = []\n",
        "    for node, data in multigraph.nodes(data=True):\n",
        "        if title in data['chapters']:\n",
        "            nodes.append(node)\n",
        "    G = nx.subgraph(multigraph, nodes)\n",
        "    nx.density(G)\n",
        "    nx.number_connected_components(G)\n",
        "    data = {'Character': title, 'Entities': len(nodes), 'Interactions' : nx.number_of_edges(G),\n",
        "            'Graph Density': nx.density(G), 'Connected Components': nx.number_connected_components(G),\n",
        "            'Average Degree': sum([v for _, v in G.degree])/len(nodes),\n",
        "            'Best Friends': (\" and \".join(sorted(G.edges(data=True), key = lambda x: x[2]['weight'], reverse=True)[0][:2]))\n",
        "            }\n",
        "    character_worlds = pd.concat([character_worlds, pd.DataFrame(data, index=[0])], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7f4e0a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "character_worlds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "obBNYUv8f7rg",
      "metadata": {
        "id": "obBNYUv8f7rg"
      },
      "outputs": [],
      "source": [
        "nx.write_gexf(multigraph, 'assets/got-multigraph.gexf')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "73bf43d7"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
