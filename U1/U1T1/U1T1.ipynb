{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U1T1 - Answers for the first assignment\n",
    "\n",
    "## Chapter 2\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "| P<sub>1</sub>(H) | P<sub>2</sub>(H) | H-H | H-T | T-H | T-T |\n",
    "| :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| 0.5 | 0.5 | 0.25 | 0.25 | 0.25 | 0.25 |\n",
    "| 0.6 | 0.7 | 0.42 | 0.18 | 0.28 | 0.12 |\n",
    "| 0.4 | 0.8 | 0.32 | 0.08 | 0.48 | 0.12 |\n",
    "| 0.1 | 0.2 | 0.02 | 0.08 | 0.18 | 0.72 |\n",
    "| 0.3 | 0.4 | 0.12 | 0.18 | 0.28 | 0.42 |\n",
    "\n",
    "For any given combination, the answer is calculated based on the positional probability for each coin as they are different. For example, in order to get the probability for a Tails - Heads outcome for the last row we have to multiply the probabilities of the first coin landing with Tails facing up and the contrary happening for the second coin. The last value is already given in the table P<sub>2</sub> = 0.4. The first one is obtained as the complement of P<sub>1</sub>(H), i.e., P<sub>1</sub>(T) =  1 - P<sub>1</sub>(H) = 0.7.\n",
    "\n",
    "From those, we obtain $P(T-H) = 0.7 * 0.4 = 0.28$\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "For starters let's grab the data available and establish what probability is desired:\n",
    "\n",
    "* Total emails received: 1605\n",
    "* Flagged emails: 963\n",
    "* Filter accuracy: 98%\n",
    "* Probability of being spam (P(S)): 0.6\n",
    "\n",
    "From the wording of the exercise we want to determine the number of 'falsely' flagged email based on the probability that an email flagged (F) as spam (S) be in fact not a spam (-S). Using Bayes' Theorem:\n",
    "\n",
    "$$P(-S|F) = \\dfrac{P(F|-S) P(-S)}{P(F)}$$\n",
    "\n",
    "The conditional probability of an genuine email being flagged is the complement of the accuracy of the filter, i.e., 0.02. Similarly, the probability of an email being genuine on its own is 1 - P(S) = 0.4. Finally, the exercise mentioned that the filter will flag 60% of the emails, so the P(F) = 0.6.\n",
    "\n",
    "Doing the math we obtain: $P(-S|F) = \\dfrac{0.02\\times0.4}{0.6} \\approx 0.01333$\n",
    "\n",
    "From that we easily obtain that something around 13 (more math accurate 12.84) emails may not be spam.\n",
    "\n",
    "### Exercise 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decipher(text: str, shift: int) -> str:\n",
    "    \"\"\"This function uses the Caesar cipher to decipher a text.\n",
    "\n",
    "    The function takes two arguments: a string and an integer. The string is the text to be deciphered,\n",
    "    and the integer is the shift value used in the Caesar cipher. The function returns the deciphered text.\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.upper()\n",
    "    result = ''\n",
    "    for char in text:\n",
    "        if char.isalpha():\n",
    "            ascii_code = ord(char)\n",
    "            ascii_code -= shift\n",
    "            if ascii_code < ord('A'):\n",
    "                ascii_code += 26\n",
    "            if ascii_code > ord('Z'):\n",
    "                ascii_code -= 26\n",
    "            result += chr(ascii_code)\n",
    "        else:\n",
    "            result += char\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = decipher('OCZ XJMMZXO VINRZM', 7)\n",
    "step2 = decipher(step1, 7)\n",
    "step3 = decipher(step2, 7)\n",
    "print('String after 1st iteration:', step1)\n",
    "print('String after 2nd iteration:', step2)\n",
    "print('String after 3rd iteration:', step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "For this exercise we will condense the information about the probabilities in a list and use the matplotlib library in python to create the visualization for mass function and cumulative distribution\n",
    "\n",
    "```python\n",
    "# Probability for outcome 'index+1'\n",
    "outcome_probabilities = [0.1, 0.15, 0.2, 0.21, 0.17, 0.09, 0.06, 0.02]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_probabilities = [0.1, 0.15, 0.2, 0.21, 0.17, 0.09, 0.06, 0.02]\n",
    "outcomes = [i for i in range(1, 9)]\n",
    "\n",
    "cumulative_probabilities = [outcome_probabilities[0]]\n",
    "\n",
    "for i in range(1, len(outcome_probabilities)):\n",
    "    cumulative_probabilities.append(outcome_probabilities[i]+cumulative_probabilities[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(15, 5)\n",
    "\n",
    "ax1.bar(outcomes, outcome_probabilities, color = '#343E4A')\n",
    "ax1.set_title('Probability mass function')\n",
    "ax1.set_xlabel('Outcome')\n",
    "ax1.set_ylabel('Probability')\n",
    "\n",
    "ax2.bar(outcomes, cumulative_probabilities, color = '#344A36')\n",
    "ax2.set_title('Cumulative distribution function')\n",
    "ax2.set_xlabel('X')\n",
    "ax2.set_ylabel('P(X <= x)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "After extracting the data from the file and creating the vector, we proceed to apply the formulas for information entropy for each vector and the mutual information for the pair as seen in the code cells below. The mutual information function from the scikit-learn library was used as it saves us from the hassling around the calculation of all the combinations of outcomes and their probability in order to evaluate the formula $$MI_{xy} = \\sum_{j\\in y}\\sum_{i\\in x}p_{ij}\\log{\\left(\\dfrac{p_{ij}}{p_ip_j}\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_entropy(arr):\n",
    "    \"\"\"This function calculates the entropy of an array using the formula H(X) = -sum(p(x) * log(p(x))).\n",
    "    \"\"\"\n",
    "    _, arr_counts = np.unique(arr, return_counts=True)\n",
    "    arr_probs = arr_counts / len(arr)\n",
    "\n",
    "    return -sum(arr_probs * np.log(arr_probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.array([\n",
    "    'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a',\n",
    "    'b', 'b', 'b', 'b', 'b', 'b',\n",
    "    'c', 'c', 'c', 'c', 'c', 'c', 'c',\n",
    "    'd', 'd', 'd', 'd', 'd', 'd', 'd', 'd',\n",
    "    'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e',\n",
    "    'f', 'f', 'f', 'f', 'f'\n",
    "])\n",
    "\n",
    "v2 = np.array([\n",
    "    'a', 's', 'a', 'd', 'd', 'f', 'f', 'f', 'f',\n",
    "    'g', 'g', 'g', 'g', 'h', 'h', 'h', 'h',\n",
    "    'j', 'j', 'j', 'k', 'j', 'g', 'h', 'h', 'g',\n",
    "    'f', 'f', 'f', 'f', 'd', 'd', \n",
    "    's', 's', 's', 's', 's', 's',\n",
    "    'a', 'a', 'a', 'a', 'a', 'd', 'f', 'g'\n",
    "])\n",
    "\n",
    "entropy_v1 = info_entropy(v1)\n",
    "entropy_v2 = info_entropy(v2)\n",
    "mi = mutual_info_score(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Entropy of v1 alone:', entropy_v1)\n",
    "print('Entropy of v2 alone:', entropy_v2)\n",
    "print('Mutual information between v1 and v2:', mi)\n",
    "print('The amount of information gained about v1 by knowing v2 saves us', mi/entropy_v1*100, '% of the bits needed to represent v1 alone.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
